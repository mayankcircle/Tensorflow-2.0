{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tensorflow Tutorial - 5 (Regularization).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO2qD/7KKy5O/pRN081/gqX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mayankcircle/Tensorflow-2.0/blob/main/Tensorflow_Tutorial_5_(Regularization).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cp3EnYLX8guH"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, regularizers\n",
        "from tensorflow.keras.datasets import cifar10"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbT-3L8f9Mm6",
        "outputId": "d8657440-904c-4114-be7c-c06e3bfeea83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.3.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyZvgMG29QYp"
      },
      "source": [
        "# Load CIFAR10 Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C29T1XL29nTB",
        "outputId": "292dbb1d-384f-47b6-bd19-0ea090e12e5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "(X_train, y_train),(X_test,y_test) = cifar10.load_data()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 4s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocWb_ZZU9f3t",
        "outputId": "fd78f727-b165-48f6-81e2-91feec806a86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 32, 32, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7uGyHS49_Ll",
        "outputId": "cfc4fc32-7355-4a25-ec3c-866a3345cdec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "set(y_train.ravel())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5EIA7O69yai"
      },
      "source": [
        "**CIFAR10 train dataset has 50000 RGB images of 32 x 32 px of 10 classes.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "265Z05j8-Vg8"
      },
      "source": [
        "We convert each value to flaot32 dtype and normalize for fast training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQS6X7vI-Vzh"
      },
      "source": [
        "X_train = X_train.astype(\"float32\") / 255.0\n",
        "X_test = X_test.astype(\"float32\") / 255.0"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLW0Pgaf-vgd"
      },
      "source": [
        "# Basic CNN Model using Sequential API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3X1iHpy-2Lu"
      },
      "source": [
        "model = keras.Sequential(\n",
        "    [\n",
        "     keras.Input(shape=(32,32,3)), # h,w,channels (Shape of 1 image). By default, data_format is \"channels_last\"\n",
        "     # 1st convolutional layer : we use 32 filters of size 3 x 3 with No padding (valid)\n",
        "     layers.Conv2D(filters=32,kernel_size=3,padding=\"valid\",activation=\"relu\",name=\"1st_CONV\"),\n",
        "     # max pool layer\n",
        "     layers.MaxPool2D(pool_size=(2,2),name=\"1st_POOL\"),\n",
        "     # 2nd convolutional layer : we use 64 filters of size 3 x 3 with No padding (valid)\n",
        "     # By default, padding is \"valid\"\n",
        "     layers.Conv2D(filters=64,kernel_size=3,activation=\"relu\",name=\"2nd_CONV\"),\n",
        "     # max pool layer\n",
        "     # By default pool_size = (2,2)\n",
        "     layers.MaxPool2D(name=\"2nd_POOL\"),\n",
        "     # 3rd convolutional layer : we use 128 filters of size 3 x 3 with No padding (valid)\n",
        "     layers.Conv2D(filters=128,kernel_size=3,activation=\"relu\", name=\"3rd_CONV\"),\n",
        "     # flatten layer\n",
        "     layers.Flatten(),\n",
        "\n",
        "     # 1st FC layer\n",
        "     layers.Dense(64,name=\"FC1\"),\n",
        "     # Output layer # 10 classes : logits\n",
        "     layers.Dense(10,name=\"FC2\")\n",
        "\n",
        "    ]\n",
        ")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlwiVC0NAzpb",
        "outputId": "4d26968c-890d-4a4a-a860-962ac382cc47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "1st_CONV (Conv2D)            (None, 30, 30, 32)        896       \n",
            "_________________________________________________________________\n",
            "1st_POOL (MaxPooling2D)      (None, 15, 15, 32)        0         \n",
            "_________________________________________________________________\n",
            "2nd_CONV (Conv2D)            (None, 13, 13, 64)        18496     \n",
            "_________________________________________________________________\n",
            "2nd_POOL (MaxPooling2D)      (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "3rd_CONV (Conv2D)            (None, 4, 4, 128)         73856     \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "FC1 (Dense)                  (None, 64)                131136    \n",
            "_________________________________________________________________\n",
            "FC2 (Dense)                  (None, 10)                650       \n",
            "=================================================================\n",
            "Total params: 225,034\n",
            "Trainable params: 225,034\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKGhPP16GHpL"
      },
      "source": [
        "model.compile(\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=keras.optimizers.Adam(lr=3e-4),\n",
        "    metrics=[\"accuracy\"]\n",
        ")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-OtzGuvGdgx",
        "outputId": "487343cc-57b1-4b65-c823-5847190a37e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "# train\n",
        "model.fit(X_train,y_train,batch_size=64,epochs=2)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "782/782 [==============================] - 3s 3ms/step - loss: 1.6156 - accuracy: 0.4156\n",
            "Epoch 2/2\n",
            "782/782 [==============================] - 3s 3ms/step - loss: 1.2876 - accuracy: 0.5427\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f7bac36f828>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vs7S162CHOSa",
        "outputId": "f964a5d5-aff1-4528-9f75-ef4c0afd8d9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "model.evaluate(X_test,y_test,batch_size=64)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "157/157 [==============================] - 0s 2ms/step - loss: 1.1962 - accuracy: 0.5813\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.196179747581482, 0.5813000202178955]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SnMNhniXtaK"
      },
      "source": [
        "# Basic CNN model using Functional API without Regularization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAsYsFFlWSuy"
      },
      "source": [
        "Build simple basic CNN model without any regularization technique"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwWjJ1pQXkni"
      },
      "source": [
        "def my_model_without_regularization():\n",
        "    inputs = keras.Input(shape=(32,32,3))\n",
        "    x = layers.Conv2D(filters=32,kernel_size=3,name=\"1st_CONV\")(inputs)\n",
        "    x = layers.BatchNormalization()(x) # we should not applying activation function before Batch Normalization, we need CONV layer RAW Output directly here\n",
        "    x = keras.activations.relu(x) # apply activation after Batch Norm\n",
        "    x = layers.MaxPool2D()(x) # by default, pooling window is 2 x 2\n",
        "    x = layers.Conv2D(filters=64,kernel_size=5,padding=\"same\",name=\"2nd_CONV\")(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = keras.activations.relu(x)\n",
        "    x = layers.Conv2D(filters=128,kernel_size=3,name=\"3rd_CONV\")(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = keras.activations.relu(x)\n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dense(64,activation=\"relu\")(x)\n",
        "    outputs = layers.Dense(10)(x)\n",
        "    \n",
        "    # create model\n",
        "    model = keras.Model(inputs=inputs, outputs = outputs)\n",
        "\n",
        "    return model\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OBcVs2Lb9tB"
      },
      "source": [
        "model = my_model_without_regularization()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xu4lN6ekeZdY",
        "outputId": "6e07a6c9-f0b8-49d3-e8db-39b1f21daf33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 656
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 32, 32, 3)]       0         \n",
            "_________________________________________________________________\n",
            "1st_CONV (Conv2D)            (None, 30, 30, 32)        896       \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 30, 30, 32)        128       \n",
            "_________________________________________________________________\n",
            "tf_op_layer_Relu (TensorFlow [(None, 30, 30, 32)]      0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 15, 15, 32)        0         \n",
            "_________________________________________________________________\n",
            "2nd_CONV (Conv2D)            (None, 15, 15, 64)        51264     \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 15, 15, 64)        256       \n",
            "_________________________________________________________________\n",
            "tf_op_layer_Relu_1 (TensorFl [(None, 15, 15, 64)]      0         \n",
            "_________________________________________________________________\n",
            "3rd_CONV (Conv2D)            (None, 13, 13, 128)       73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 13, 13, 128)       512       \n",
            "_________________________________________________________________\n",
            "tf_op_layer_Relu_2 (TensorFl [(None, 13, 13, 128)]     0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 21632)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 64)                1384512   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                650       \n",
            "=================================================================\n",
            "Total params: 1,512,074\n",
            "Trainable params: 1,511,626\n",
            "Non-trainable params: 448\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55knwlqkbTVA"
      },
      "source": [
        "model.compile(\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=keras.optimizers.Adam(lr=3e-4),\n",
        "    metrics=[\"accuracy\"]\n",
        ")"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2zEVH8KcE_8",
        "outputId": "ce55714e-353e-49d3-e414-ba09e623aad0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        }
      },
      "source": [
        "# train\n",
        "model.fit(X_train,y_train,batch_size=64,epochs=10)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 0.7268 - accuracy: 0.7487\n",
            "Epoch 2/10\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 0.6228 - accuracy: 0.7831\n",
            "Epoch 3/10\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 0.5295 - accuracy: 0.8145\n",
            "Epoch 4/10\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 0.4485 - accuracy: 0.8474\n",
            "Epoch 5/10\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 0.3785 - accuracy: 0.8704\n",
            "Epoch 6/10\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 0.3124 - accuracy: 0.8947\n",
            "Epoch 7/10\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 0.2595 - accuracy: 0.9121\n",
            "Epoch 8/10\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 0.2084 - accuracy: 0.9315\n",
            "Epoch 9/10\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 0.1732 - accuracy: 0.9447\n",
            "Epoch 10/10\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 0.1368 - accuracy: 0.9575\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f7ba4f6fcc0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVr5v4EucG3u",
        "outputId": "cf964d4d-0e63-42ef-9919-d4b18f4e13f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "model.evaluate(X_test,y_test,batch_size=64)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "157/157 [==============================] - 0s 3ms/step - loss: 1.0464 - accuracy: 0.7308\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.0463511943817139, 0.7307999730110168]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mzg0YbiVWbA_"
      },
      "source": [
        "The gap between training and test accuracy is large, it means it is overfitted on training data. We can solve this using **REgularization**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jijNSgE_W4Kz"
      },
      "source": [
        "# Basic CNN model using Functional API with Regularization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "us7P0B4mXENJ"
      },
      "source": [
        "we can add regularizer seperately in layers. Here, we are using L2 Regularizer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AM4Vnjn-YMpH"
      },
      "source": [
        "we are also using dropout in FC Layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzs7i2hhYbX1"
      },
      "source": [
        "As we had already **BatchNormalization**, **the purpose of Batch NOrmalization is to normalize weights to have faster training but it also has some regularization effect**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqMB-wBcWtKZ"
      },
      "source": [
        "def my_model_with_regularization():\n",
        "    inputs = keras.Input(shape=(32,32,3))\n",
        "    x = layers.Conv2D(filters=32,kernel_size=3,name=\"1st_CONV\",kernel_regularizer=regularizers.l2(0.01))(inputs)\n",
        "    x = layers.BatchNormalization()(x) # we should not applying activation function before Batch Normalization, we need CONV layer RAW Output directly here\n",
        "    x = keras.activations.relu(x) # apply activation after Batch Norm\n",
        "    x = layers.MaxPool2D()(x) # by default, pooling window is 2 x 2\n",
        "    x = layers.Conv2D(filters=64,kernel_size=5,padding=\"same\",name=\"2nd_CONV\", kernel_regularizer=regularizers.l2(0.01))(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = keras.activations.relu(x)\n",
        "    x = layers.Conv2D(filters=128,kernel_size=3,name=\"3rd_CONV\",kernel_regularizer=regularizers.l2(0.01))(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = keras.activations.relu(x)\n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dense(64,activation=\"relu\", kernel_regularizer=regularizers.l2(0.01))(x)\n",
        "    x = layers.Dropout(0.5)(x) # dropout layer\n",
        "    outputs = layers.Dense(10)(x)\n",
        "    \n",
        "    # create model\n",
        "    model = keras.Model(inputs=inputs, outputs = outputs)\n",
        "\n",
        "    return model"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tek3KrkdZqEO"
      },
      "source": [
        "model = my_model_with_regularization()"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56ITDdNQZqEj",
        "outputId": "3fb0cfcb-f2ac-4fa1-d938-f69a3d9144a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 692
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_4 (InputLayer)         [(None, 32, 32, 3)]       0         \n",
            "_________________________________________________________________\n",
            "1st_CONV (Conv2D)            (None, 30, 30, 32)        896       \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 30, 30, 32)        128       \n",
            "_________________________________________________________________\n",
            "tf_op_layer_Relu_4 (TensorFl [(None, 30, 30, 32)]      0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 15, 15, 32)        0         \n",
            "_________________________________________________________________\n",
            "2nd_CONV (Conv2D)            (None, 15, 15, 64)        51264     \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 15, 15, 64)        256       \n",
            "_________________________________________________________________\n",
            "tf_op_layer_Relu_5 (TensorFl [(None, 15, 15, 64)]      0         \n",
            "_________________________________________________________________\n",
            "3rd_CONV (Conv2D)            (None, 13, 13, 128)       73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 13, 13, 128)       512       \n",
            "_________________________________________________________________\n",
            "tf_op_layer_Relu_6 (TensorFl [(None, 13, 13, 128)]     0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 21632)             0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 64)                1384512   \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                650       \n",
            "=================================================================\n",
            "Total params: 1,512,074\n",
            "Trainable params: 1,511,626\n",
            "Non-trainable params: 448\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1WNhivMZqEw"
      },
      "source": [
        "model.compile(\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=keras.optimizers.Adam(lr=3e-4),\n",
        "    metrics=[\"accuracy\"]\n",
        ")"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGnuYO0ZZvbg"
      },
      "source": [
        "Since , we are using Dropout layer, we need to do more epochs in training because we are dropping connections randomly and it needs more iteration to train the network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCtq9g74ZqE1",
        "outputId": "08c8907e-942e-4d0e-808d-b5ab140185b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# large epochs due to dropout\n",
        "model.fit(X_train,y_train,batch_size=64,epochs=150)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 3.1128 - accuracy: 0.1650\n",
            "Epoch 2/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 2.1527 - accuracy: 0.2481\n",
            "Epoch 3/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.9350 - accuracy: 0.3058\n",
            "Epoch 4/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.8456 - accuracy: 0.3332\n",
            "Epoch 5/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.7688 - accuracy: 0.3574\n",
            "Epoch 6/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.7373 - accuracy: 0.3860\n",
            "Epoch 7/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.6976 - accuracy: 0.4046\n",
            "Epoch 8/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.6866 - accuracy: 0.4119\n",
            "Epoch 9/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.6675 - accuracy: 0.4203\n",
            "Epoch 10/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.6586 - accuracy: 0.4225\n",
            "Epoch 11/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.6449 - accuracy: 0.4259\n",
            "Epoch 12/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.6367 - accuracy: 0.4351\n",
            "Epoch 13/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.6271 - accuracy: 0.4358\n",
            "Epoch 14/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.6217 - accuracy: 0.4405\n",
            "Epoch 15/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.6135 - accuracy: 0.4456\n",
            "Epoch 16/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.6006 - accuracy: 0.4481\n",
            "Epoch 17/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.5886 - accuracy: 0.4443\n",
            "Epoch 18/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.5764 - accuracy: 0.4510\n",
            "Epoch 19/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.5669 - accuracy: 0.4555\n",
            "Epoch 20/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.5617 - accuracy: 0.4633\n",
            "Epoch 21/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.5642 - accuracy: 0.4626\n",
            "Epoch 22/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.5441 - accuracy: 0.4687\n",
            "Epoch 23/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.5439 - accuracy: 0.4704\n",
            "Epoch 24/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.5370 - accuracy: 0.4734\n",
            "Epoch 25/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.5254 - accuracy: 0.4787\n",
            "Epoch 26/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.5239 - accuracy: 0.4832\n",
            "Epoch 27/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.5128 - accuracy: 0.4878\n",
            "Epoch 28/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.5125 - accuracy: 0.4892\n",
            "Epoch 29/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.5028 - accuracy: 0.4923\n",
            "Epoch 30/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.5123 - accuracy: 0.4927\n",
            "Epoch 31/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.4958 - accuracy: 0.4930\n",
            "Epoch 32/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.4976 - accuracy: 0.4932\n",
            "Epoch 33/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.4956 - accuracy: 0.4934\n",
            "Epoch 34/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.5010 - accuracy: 0.4930\n",
            "Epoch 35/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.4850 - accuracy: 0.5010\n",
            "Epoch 36/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.4740 - accuracy: 0.5044\n",
            "Epoch 37/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.4759 - accuracy: 0.5013\n",
            "Epoch 38/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.4813 - accuracy: 0.5031\n",
            "Epoch 39/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.4682 - accuracy: 0.5052\n",
            "Epoch 40/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.4616 - accuracy: 0.5111\n",
            "Epoch 41/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.4682 - accuracy: 0.5082\n",
            "Epoch 42/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.4643 - accuracy: 0.5095\n",
            "Epoch 43/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.4570 - accuracy: 0.5124\n",
            "Epoch 44/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.4629 - accuracy: 0.5127\n",
            "Epoch 45/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.4648 - accuracy: 0.5118\n",
            "Epoch 46/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.4576 - accuracy: 0.5157\n",
            "Epoch 47/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.4504 - accuracy: 0.5184\n",
            "Epoch 48/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.4509 - accuracy: 0.5177\n",
            "Epoch 49/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.4464 - accuracy: 0.5204\n",
            "Epoch 50/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.4442 - accuracy: 0.5236\n",
            "Epoch 51/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.4374 - accuracy: 0.5277\n",
            "Epoch 52/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.4436 - accuracy: 0.5255\n",
            "Epoch 53/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.4416 - accuracy: 0.5258\n",
            "Epoch 54/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.4398 - accuracy: 0.5309\n",
            "Epoch 55/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.4298 - accuracy: 0.5334\n",
            "Epoch 56/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.4311 - accuracy: 0.5330\n",
            "Epoch 57/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.4358 - accuracy: 0.5303\n",
            "Epoch 58/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.4288 - accuracy: 0.5350\n",
            "Epoch 59/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.4261 - accuracy: 0.5343\n",
            "Epoch 60/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.4295 - accuracy: 0.5356\n",
            "Epoch 61/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.4261 - accuracy: 0.5363\n",
            "Epoch 62/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.4234 - accuracy: 0.5377\n",
            "Epoch 63/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.4236 - accuracy: 0.5377\n",
            "Epoch 64/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.4266 - accuracy: 0.5383\n",
            "Epoch 65/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.4164 - accuracy: 0.5420\n",
            "Epoch 66/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.4207 - accuracy: 0.5404\n",
            "Epoch 67/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.4164 - accuracy: 0.5426\n",
            "Epoch 68/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.4290 - accuracy: 0.5406\n",
            "Epoch 69/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.4136 - accuracy: 0.5467\n",
            "Epoch 70/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.4156 - accuracy: 0.5449\n",
            "Epoch 71/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.4141 - accuracy: 0.5440\n",
            "Epoch 72/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.4164 - accuracy: 0.5435\n",
            "Epoch 73/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.4022 - accuracy: 0.5479\n",
            "Epoch 74/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.4112 - accuracy: 0.5458\n",
            "Epoch 75/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.4100 - accuracy: 0.5476\n",
            "Epoch 76/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.4084 - accuracy: 0.5463\n",
            "Epoch 77/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.4048 - accuracy: 0.5460\n",
            "Epoch 78/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.4162 - accuracy: 0.5462\n",
            "Epoch 79/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.4073 - accuracy: 0.5469\n",
            "Epoch 80/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.3941 - accuracy: 0.5516\n",
            "Epoch 81/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.4052 - accuracy: 0.5475\n",
            "Epoch 82/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.4055 - accuracy: 0.5487\n",
            "Epoch 83/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.3923 - accuracy: 0.5535\n",
            "Epoch 84/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.4037 - accuracy: 0.5506\n",
            "Epoch 85/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.3967 - accuracy: 0.5536\n",
            "Epoch 86/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.4039 - accuracy: 0.5526\n",
            "Epoch 87/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.4030 - accuracy: 0.5493\n",
            "Epoch 88/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.4042 - accuracy: 0.5484\n",
            "Epoch 89/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.3905 - accuracy: 0.5533\n",
            "Epoch 90/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.3906 - accuracy: 0.5549\n",
            "Epoch 91/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.4025 - accuracy: 0.5495\n",
            "Epoch 92/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.3952 - accuracy: 0.5534\n",
            "Epoch 93/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.3924 - accuracy: 0.5546\n",
            "Epoch 94/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.3855 - accuracy: 0.5547\n",
            "Epoch 95/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.4005 - accuracy: 0.5534\n",
            "Epoch 96/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.3931 - accuracy: 0.5527\n",
            "Epoch 97/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.3894 - accuracy: 0.5567\n",
            "Epoch 98/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.3948 - accuracy: 0.5546\n",
            "Epoch 99/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.3948 - accuracy: 0.5529\n",
            "Epoch 100/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.3953 - accuracy: 0.5518\n",
            "Epoch 101/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.3943 - accuracy: 0.5532\n",
            "Epoch 102/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.3944 - accuracy: 0.5552\n",
            "Epoch 103/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.3873 - accuracy: 0.5542\n",
            "Epoch 104/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.3933 - accuracy: 0.5539\n",
            "Epoch 105/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.3858 - accuracy: 0.5546\n",
            "Epoch 106/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.3896 - accuracy: 0.5552\n",
            "Epoch 107/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.3830 - accuracy: 0.5593\n",
            "Epoch 108/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.3843 - accuracy: 0.5586\n",
            "Epoch 109/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.3862 - accuracy: 0.5599\n",
            "Epoch 110/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.3873 - accuracy: 0.5595\n",
            "Epoch 111/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.3797 - accuracy: 0.5611\n",
            "Epoch 112/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.3743 - accuracy: 0.5641\n",
            "Epoch 113/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.3860 - accuracy: 0.5598\n",
            "Epoch 114/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.3815 - accuracy: 0.5613\n",
            "Epoch 115/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.3799 - accuracy: 0.5622\n",
            "Epoch 116/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.3781 - accuracy: 0.5635\n",
            "Epoch 117/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.3799 - accuracy: 0.5624\n",
            "Epoch 118/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.3746 - accuracy: 0.5647\n",
            "Epoch 119/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.3765 - accuracy: 0.5635\n",
            "Epoch 120/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.3721 - accuracy: 0.5621\n",
            "Epoch 121/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.3853 - accuracy: 0.5608\n",
            "Epoch 122/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.3740 - accuracy: 0.5636\n",
            "Epoch 123/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.3780 - accuracy: 0.5626\n",
            "Epoch 124/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.3758 - accuracy: 0.5627\n",
            "Epoch 125/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.3808 - accuracy: 0.5641\n",
            "Epoch 126/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.3736 - accuracy: 0.5649\n",
            "Epoch 127/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.3759 - accuracy: 0.5667\n",
            "Epoch 128/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.3745 - accuracy: 0.5643\n",
            "Epoch 129/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.3711 - accuracy: 0.5671\n",
            "Epoch 130/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.3868 - accuracy: 0.5582\n",
            "Epoch 131/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.3677 - accuracy: 0.5680\n",
            "Epoch 132/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.3715 - accuracy: 0.5636\n",
            "Epoch 133/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.3731 - accuracy: 0.5641\n",
            "Epoch 134/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.3758 - accuracy: 0.5644\n",
            "Epoch 135/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.3748 - accuracy: 0.5655\n",
            "Epoch 136/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.3783 - accuracy: 0.5639\n",
            "Epoch 137/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.3655 - accuracy: 0.5689\n",
            "Epoch 138/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.3609 - accuracy: 0.5708\n",
            "Epoch 139/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.3674 - accuracy: 0.5657\n",
            "Epoch 140/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.3676 - accuracy: 0.5699\n",
            "Epoch 141/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.3713 - accuracy: 0.5663\n",
            "Epoch 142/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.3705 - accuracy: 0.5671\n",
            "Epoch 143/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.3672 - accuracy: 0.5681\n",
            "Epoch 144/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.3619 - accuracy: 0.5716\n",
            "Epoch 145/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.3668 - accuracy: 0.5659\n",
            "Epoch 146/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.3741 - accuracy: 0.5685\n",
            "Epoch 147/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.3549 - accuracy: 0.5729\n",
            "Epoch 148/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.3622 - accuracy: 0.5756\n",
            "Epoch 149/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.3600 - accuracy: 0.5752\n",
            "Epoch 150/150\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 1.3590 - accuracy: 0.5755\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f7ba43b57b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCnKzAAyZqE6",
        "outputId": "aa32e150-6c8d-4356-ea3e-d8d173262744",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "model.evaluate(X_test,y_test,batch_size=64)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "157/157 [==============================] - 0s 3ms/step - loss: 1.2204 - accuracy: 0.6521\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.220400333404541, 0.6521000266075134]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHXtUNTjlRki"
      },
      "source": [
        "It is now looking more underfit. MOre learing is required."
      ]
    }
  ]
}